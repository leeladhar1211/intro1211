----optimizing plsql performance
Profiling, Tracing, Debugging Using Oracle Supplied Packages
DBMS_PROFILER-> Analyze each program statement,Collect runtime statistics
DBMS_TRACE->Trace program and subprogram execution steps,Collect runtime statistics
DBMS_HPROF->Hierarchical Profiler,Analyze SQL and PL/SQL statements separately,Generate HTML Reports
DBMS_DEBUG->Debug server side code,The PLSQL_DEBUG parameter,Using the COMPILE DEBUG option,Setting breakpoints,Analyzing variables
-------------------------------------------
TUNE ACCESS TO CODE AND DATA IN THE SGA
Before your code can be executed (and perhaps run too slowly), it must be loaded into the SGA of the Oracle instance. 
Use the most aggressive compiler optimization level possible
Oracle Database 10g introduced an optimizing compiler for PL/SQL programs. The default optimization level of 2 in that release took the most aggressive approach possible in terms of transforming your code to make it run faster oracle Database 11g and later support an even higher optimization level of 3
DBMS_PROFILER
This built-in package allows you to turn on execution profiling in a session. Then, when you run your code, the Oracle database uses tables to keep track of detailed information about how long each line in your code took to execute
DBMS_HPROF (hierarchical profiler)
Oracle Database 11g introduced a hierarchical profiler that makes it easier to roll performance results up through the execution call stack. DBMS_PROFILER provides “flat” data about performance, which makes it difficult to answer questions like “How much time altogether is spent in the ADD_ITEM procedure?” The hierarchical profiler makes it easy to answer such questions.
-------------------
PROFILING means :identifying the queries that have the longest duration.understand where their code is spending the most time, so they can detect and optimize it. The profiling utility allows Oracle to collect data in memory structures and then dumps it into tables as application code is executed.

TRACING : Oracle Trace includes: SQL statements and statistics on the frequency, duration, and resources used for all parse, execution, and fetch events; Execution plan details; Logical and physical database transactions, including the frequency, duration, and resources used by ..

DBMS_PROFILER package which is used to trace run time behavior the PL/SQL code. It is not gives any solution to the poor performance PL/SQL code. It is not inbuilt package i.e., this package is not available in default oracle installation. we need to install this package separately.
DBMS_PROFILER
To use the profiler in the old mode, you must first run the script proftab.sqlin the schema used to execute your procedure / function. This script creates the following tables:

PLSQL_PROFILER_RUNS list all the executions of the profiler
colms available: runid ,related_run ,run_owner ,run_date ,run_comment ,run_total_time 
PLSQL_PROFILER_UNITS list all measured threads
PLSQL_PROFILER_DATA stores the details of information stored by thread

grant execute on DBMS_PROFILER to demo;<br /><br />connect demo/demo<br />@?/rdbms/admin/proftab.sql

grant execute on DBMS_PROFILER to demo;<br /><br />connect demo/demo<br />@?/rdbms/admin/proftab.sql

Use DBMS_PROFILER requires to use the same session as the activation; this is not usually a problem since you can add the code in the header / end of a PL / SQL procedure or via a login / logout trigger. In the example below we just run the profiling before launching our code and disable it (writes the data in the reference tables) once the procedure is executed:
Points to remember:

1) Profiler is needed to setup all development schemas. If we setup the profiler in system schema only, we cannot use this.

2) Total time is mentioned in nanoseconds. We will convert nanoseconds into seconds using run_toal_time/1000000000.
-------------------------
DBMS_HPROF
DBMS_HPROF resumes for a lot of the previous operation. To start, you will need to run the script dbmshptab.sql that contains the creation of the tables:

DBMSHP_RUNS contains information about executions
DBMSHP_FUNCTION_INFO contains the runtime elements
DBMSHP_PARENT_CHILD_INFO stores the details of the information stored per execution unit resulting from the analysis operation

As for DBMS_PROFILER, you must use the same session for the procedure and activation; the same possibilities of triggers or encapsulation are to be implemented for a capture in production. Note that in this case, the data is stored as a trace file that can later be used without the tables DBMSHP_FUNCTION_INFOand DBMSHP_PARENT_CHILD_INFOare fed. The documentation describes in some detail the format of this trace file :
At this moment, you have 2 options:
either, generate all the elements of your report and query the tables described previously:
or, use the utility provided to extract all of this information as HTML files. Here is an example of use:
--------------------------
The DBMS_TRACE package provides an API to allow the actions of PL/SQL programs to be traced. The scope and volume of the tracing is user configurable. This package can be used in conjunction with the DBMS_PROFILER package to identify performance bottlenecks.
The first step is to install the tables which will hold the trace data. @$ORACLE_HOME/rdbms/admin/tracetab.sql
-----increasing performance by bulkcollect,forall
What is BULK COLLECT?
BULK COLLECT reduces context switches between SQL and PL/SQL engine and allows SQL engine to fetch the records at once.

Oracle PL/SQL provides the functionality of fetching the records in bulk rather than fetching one-by-one. This BULK COLLECT can be used in 'SELECT' statement to populate the records in bulk or in fetching the cursor in bulk. Since the BULK COLLECT fetches the record in BULK, the INTO clause should always contain a collection type variable. The main advantage of using BULK COLLECT is it increases the performance by reducing the interaction between database and PL/SQL engine.

SELECT <columnl> BULK COLLECT INTO bulk_varaible FROM <table name>;  --In the above syntax, BULK COLLECT is used in collect the data from 'SELECT' and 'FETCH' statement.
FETCH <cursor_name> BULK COLLECT INTO <bulk_varaible >;

FORALL Clause
The FORALL allows to perform the DML operations on data in bulk. It is similar to that of FOR loop statement except in FOR loop things happen at the record-level whereas in FORALL there is no LOOP concept. Instead the entire data present in the given range is processed at the same time.

Syntax:

FORALL <loop_variable>in<lower range> .. <higher range> 

<DML operations>;
In the above syntax, the given DML operation will be executed for the entire data that is present between lower and higher range.

LIMIT Clause
The bulk collect concept loads the entire data into the target collection variable as a bulk i.e. the whole data will be populated into the collection variable in a single-go. But this is not advisable when the total record that needs to be loaded is very large, because when PL/SQL tries to load the entire data it consumes more session memory. Hence, it is always good to limit the size of this bulk collect operation.
Syntax:

FETCH <cursor_name> BULK COLLECT INTO <bulk_variable> LIMIT <size>;

BULK COLLECT Attributes
Similar to cursor attributes BULK COLLECT has %BULK_ROWCOUNT(n) that returns the number of rows affected in the nth DML statement of the FORALL statement, i.e. it will give the count of records affected in the FORALL statement for every single value from the collection variable. The term 'n' indicates the sequence of value in the collection, for which the row count is needed.

FOR i IN l_array.first .. l_array.last LOOP
    DBMS_OUTPUT.put_line('Element: ' || RPAD(l_array(i), 15, ' ') ||
      ' Rows affected: ' || SQL%BULK_ROWCOUNT(i));
  END LOOP;
  
Example 1: In this example, we will project all the employee name from emp table using BULK COLLECT and we are also going to increase the salary of all the employees by 5000 using FORALL.
DECLARE
CURSOR guru99_det IS SELECT emp_name FROM emp;
TYPE lv_emp_name_tbl IS TABLE OF VARCHAR2(50);
lv_emp_name lv_emp_name_tbl;
BEGIN
OPEN guru99_det;
FETCH guru99_det BULK COLLECT INTO lv_emp_name LIMIT 5000;
FOR c_emp_name IN lv_emp_name.FIRST .. lv_emp_name.LAST
LOOP
Dbms_output.put_line(‘Employee Fetched:‘||c_emp_name);
END LOOP:
FORALL i IN lv_emp_name.FIRST .. lv emp_name.LAST
UPDATE emp SET salaiy=salary+5000 WHERE emp_name=lv_emp_name(i);
COMMIT;	
Dbms_output.put_line(‘Salary Updated‘);
CLOSE guru99_det;
END;
/
Code Explanation:

Code line 2: Declaring the cursor guru99_det for statement 'SELECT emp_name FROM emp'.
Code line 3: Declaring lv_emp_name_tbl as table type of VARCHAR2(50)
Code line 4: Declaring lv_emp_name as lv_emp_name_tbl type.
Code line 6: Opening the cursor.
Code line 7: Fetching the cursor using BULK COLLECT with the LIMIT size as 5000 intl lv_emp_name variable.
Code line 8-11: Setting up FOR loop to print all the record in the collection lv_emp_name.
Code line 12: Using FORALL updating the salary of all the employee by 5000.
Code line 14: Committing the transaction.

SAVE EXCEPTIONS and SQL%BULK_EXCEPTION

We saw how the FORALL syntax allows us to perform bulk DML operations, but what happens if one of those individual operations results in an exception? If there is no exception handler, all the work done by the current bulk operation is rolled back. If there is an exception handler, the work done prior to the exception is kept, but no more processing is done. Neither of these situations is very satisfactory, so instead we should use the SAVE EXCEPTIONS clause to capture the exceptions and allow us to continue past them. We can subsequently look at the exceptions by referencing the SQL%BULK_EXCEPTION cursor attribute. To see this in action create the following table.

BULK BINDS AND TRIGGERS
For bulk updates and deletes the timing points remain unchanged. Each row in the collection triggers a before statement, before row, after row and after statement timing point. For bulk inserts, the statement level triggers only fire at the start and the end of the the whole bulk operation, rather than for each row of the collection. This can cause some confusion if you are relying on the timing points from row-by-row processing.

----indices of and values of 
The INDICES OF clause allows a bulk operation on a sparse collection by removing the reference to specific elements. In addition, upper and lower bounds can be specified using the BETWEEN clause.  Allowable syntaxes include:

FORALL index IN INDICES OF collection
FORALL index IN INDICES OF collection BETWEEN start AND end

In a FORALL statement, the VALUES OF clause allows the values of one collection to be used as index pointers to another collection, as shown in the forall_values_of.sql script.


Sparse. A collection is sparse if there is at least one index value between the lowest and highest defined index values that is not defined. For example, a sparse collection has an element assigned to index value 1 and another to index value 10 but nothing in between. The opposite of a sparse collection is a dense one.

Types of Collections
Collections were first introduced in Oracle7 Server and have been enhanced in several ways through the years and across Oracle Database versions. There are now three types of collections to choose from, each with its own set of characteristics and each best suited to a different circumstance.

Associative array. The first type of collection available in PL/SQL, this was originally called a “PL/SQL table” and can be used only in PL/SQL blocks. Associative arrays can be sparse or dense and can be indexed by integer or string.

Nested table. Added in Oracle8 Database, the nested table can be used in PL/SQL blocks, in SQL statements, and as the datatype of columns in tables. Nested tables can be sparse but are almost always dense. They can be indexed only by integer. You can use the MULTISET operator to perform set operations and to perform equality comparisons on nested tables.

Varray. Added in Oracle8 Database, the varray (variable-size array) can be used in PL/SQL blocks, in SQL statements, and as the datatype of columns in tables. Varrays are always dense and indexed by integer. When a varray type is defined, you must specify the maximum number of elements allowed in a collection declared with that type
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
The dbms_warning built-in package is used in 11g to manipulate the default PL/SQL warning messages, in conjunction with the new 11g plsql_warnings parameter.

According to Steve Feuerstein, "dbms_warning was designed to be used in install scripts in which you might need to disable a certain warning, or treat a warning as an error, for individual program units being compiled."

Dr. Tim Hall shows this example of the dbms_warning package:

ALTER SESSION SET PLSQL_WARNINGS='ENABLE:ALL';

CREATE OR REPLACE PROCEDURE test_warnings AS
  l_dummy  VARCHAR2(10) := '1';
BEGIN
  IF 1=1 THEN
    SELECT '2'
    INTO   l_dummy
    FROM   dual;
  ELSE
    RAISE_APPLICATION_ERROR(-20000, 'l_dummy != 1!');
  END IF;
END;
/

SP2-0804: Procedure created with compilation warnings

SHOW ERRORS

LINE/COL ERROR
-------- ---------------------------
9/5      PLW-06002: Unreachable code

----------------------------------------------------------------------------------------------------------------------------------
Enable code optimization technique using PLSQL_OPTIMIZE_LEVEL
The parameter was introduced in Oracle 10g to decide the level of optimization applicable for a program unit during compilation. The optimization during compilation means the removal of dead code, code movements to optimize iterative constructs, inline the calls if required, and choosing the best philosophy to optimally compile the program.

It is set by the DBA at system, session and object level. It is the object compilation parameter which is retained by the object library unit. It can be queried in USER_PLSQL_OBJECT_SETTINGS view for a given object.

Prior to 11g release, the parameter could accommodate only three valid values i.e. 0, 1, and 2. Oracle 11g introduced an additional optimization level 3. By default, the parameter value is 2. The compiler’s effort is directly proportional to the parameter value i.e. more the value, more the compiler’s effort.
ALTER SESSION SET PLSQL_OPTIMIZE_LEVEL = 1

PLSQL_OPTIMIZE_LEVEL:
Level 0: no compiler optimizations (PL/SQL compiled as is);
Level 1: high-level optimizations (such as moving constants out of loops);
Level 2: default level. Aggressive optimizations (such as rewriting cursor-for-loops as array fetches) and in 11g, also inlining any subprograms that we request with PRAGMA INLINE;
Level 3: most aggressive level: New in 11g, these will inline all subprograms where possible (excluding those contained in built-in packages).
-----------------------------------------------------------------------------------

11g PL/SQL Subprogram Inlining and Compound Triggers

Subprogram inlining has been around in optimizing compilers for a long time.  With the 11g database, Oracle has included this feature in PL/SQL.  Basically subprogram inlining is where the program calls a subprogram (procedure or function) and the compiler places the actual subprogram code in place of the call to that code.  Since the subprogram code is in the main code (INLINE), the overhead of calling that subprogram is saved.  This allows you to program using a modular method, producing readable, maintainable source code, while the compiler optimizes the code to make it more efficient.

If the PLSQL_OPTIMIZER_LEVEL is set to 3 the compiler will automatically try and inline subprograms.  If the PLSQL_OPTIMIZER_LEVEL is set to 2 (the default) it will not automatically try and inline subprograms.  So the PRAGMA INLINE is used to tell the optimizer to inline or not to inline depending on the setting of the optimizer level.

The PRAGMA INLINE statement is used to tell the optimizer if the following statements subprograms should be placed inline or not.

In the example below, the PARGMA INLINE is used to inline the first function call but not the second.  The PARGMA INLINE statement only effects the statement following PARGMA INLINE statement.

create or replace package inline_demo is
  procedure runtest;
end;
/

create or replace package body inline_demo as 

function newadd(n1 number) return number
as
 n11 number;
begin
  n11:= n1+n1;
  return n11;
end;

procedure runtest as  
  n2 number := 0;
begin
  n2 := 3;
  dbms_output.put('Num: '||n2);
  PRAGMA INLINE (newadd,'YES');
  n2 := newadd(n2);
  dbms_output.put('  Num2: '||n2);
  PRAGMA INLINE (newadd,'NO');
  n2 := newadd(n2);
  dbms_output.put_line('  Num3: '||n2);
end;

end inline_demo;
/

SQL> exec inline_demo.runtest;
Num: 3  Num2: 6  Num3: 12

PL/SQL procedure successfully completed.

The PARGMA INLINE impacts all calls in the statement that follows it. If the statement is a loop, then all calls to the subprogram in the loop is placed inline.

Inlining subprogram code is a common and powerful code optimization capability that should be used when possible.  Placing the PLSQL_OPTIMIZER_LEVEL parameter to 3 will automate the code inline process.
----------------------------------------------------------------------------------------------------------
When to use Compound Triggers
The compound trigger is useful when you want to accumulate facts that characterize the “for each row” changes and then act on them as a body at “after statement” time. Two popular reasons to use compound trigger are:

To accumulate rows for bulk-insertion. We will later see an example for this.
To avoid the infamous ORA-04091: mutating-table error
PL/SQL Compound Triggers

A compound trigger is a PL/SQL trigger that contains multiple actions that take place at different timing points.  The timing points for a compound trigger on a table are:

BEFORE STATEMENT  - before trigger statement executes
AFTER STATEMENT     - after the trigger statement executes
BEFORE EACH ROW    - before each row the statement impacts
AFTER EACH ROW       - after each row the statement impacts

If the compound trigger is on a view, the only timing point is:

INSTEAD OF EACH ROW

The real advantage of the compound trigger is that the trigger definition has a declaration section and variable state is maintained through each timing section from the firing of the trigger to completion.  The statements at each timing point have some restrictions.  The statement must be DML and can contain Inserting, Updating, Deleting and Applying.  If the statement at the timing point does not impact any rows, the before statement and after statement do not fire.  The compound trigger body does not have an initialization block and thus no exception block.  All exceptions must be handled in the section that raised the exception.  You cannot pass execution from one section to another using an exception handler or a GOTO statement.  The references to :OLD, :NEW or :PARENT cannot be used in the declaration section, BEFORE STATEMENT or AFTER STATEMENT.  Only the BEFORE EACH ROW section can change the :NEW value.

The order of firing within a compound trigger is defined, however like simple triggers, the order of firing of multiple triggers (compound or simple) is not guaranteed. The order of firing multiple triggers can be defined using the FOLLOWS statement.  The FOLLOWS option is explained in the next section.

Lets look at a compound trigger. The following code creates a test table and a compound trigger that fires for each timing point associated with insert, update and delete statements. The triggering actions are logged in a PL/SQL table defined in the global declaration section. The final timing point for each statement prints out the content of the PL/SQL table to show that the variable state has been maintained throughout the lifetime of the statement.

The example courtesy of Dr. Tim Hall (www.oracle-base.com)

CREATE TABLE compound_trigger_test (
  id           NUMBER,
  description  VARCHAR2(50)
);
CREATE OR REPLACE TRIGGER compound_trigger_test_trg
  FOR INSERT OR UPDATE OR DELETE ON compound_trigger_test
    COMPOUND TRIGGER 

  -- Global declaration.

  TYPE t_tab IS TABLE OF VARCHAR2(50);
  l_tab t_tab := t_tab(); 

  BEFORE STATEMENT IS

  BEGIN
    l_tab.extend;
    CASE
      WHEN INSERTING THEN
        l_tab(l_tab.last) := 'BEFORE STATEMENT - INSERT';
      WHEN UPDATING THEN
        l_tab(l_tab.last) := 'BEFORE STATEMENT - UPDATE';
      WHEN DELETING THEN
        l_tab(l_tab.last) := 'BEFORE STATEMENT - DELETE';
    END CASE;
  END BEFORE STATEMENT; 

  BEFORE EACH ROW IS
  BEGIN
    l_tab.extend;
    CASE
      WHEN INSERTING THEN
        l_tab(l_tab.last) := 'BEFORE EACH ROW - INSERT (new.id=' || :new.id ||
')';
      WHEN UPDATING THEN
        l_tab(l_tab.last) := 'BEFORE EACH ROW - UPDATE (new.id=' || :new.id ||
' old.id=' || :old.id || ')';
      WHEN DELETING THEN
        l_tab(l_tab.last) := 'BEFORE EACH ROW - DELETE (old.id=' || :old.id ||
')';
    END CASE;
  END BEFORE EACH ROW; 

  AFTER EACH ROW IS
  BEGIN
    l_tab.extend;
    CASE
      WHEN INSERTING THEN
        l_tab(l_tab.last) := 'AFTER EACH ROW - INSERT (new.id=' || :new.id ||
')';
      WHEN UPDATING THEN
        l_tab(l_tab.last) := 'AFTER EACH ROW - UPDATE (new.id=' || :new.id ||
' old.id=' || :old.id || ')';
      WHEN DELETING THEN
        l_tab(l_tab.last) := 'AFTER EACH ROW - DELETE (old.id=' || :old.id ||
')';
    END CASE;
  END AFTER EACH ROW; 

  AFTER STATEMENT IS
  BEGIN
    l_tab.extend;
    CASE
      WHEN INSERTING THEN
        l_tab(l_tab.last) := 'AFTER STATEMENT - INSERT';
      WHEN UPDATING THEN
        l_tab(l_tab.last) := 'AFTER STATEMENT - UPDATE';
      WHEN DELETING THEN
        l_tab(l_tab.last) := 'AFTER STATEMENT - DELETE';
    END CASE;   

    FOR i IN l_tab.first .. l_tab.last LOOP
      DBMS_OUTPUT.put_line(l_tab(i));
    END LOOP;
    l_tab.delete;
  END AFTER STATEMENT; 

END compound_trigger_test_trg;
/

By issuing several insert, update and delete statements against the test table we can see that the compound trigger is working as expected.

SQL> SET SERVEROUTPUT ON
SQL> INSERT INTO compound_trigger_test VALUES (1, 'ONE');
BEFORE STATEMENT - INSERT
BEFORE EACH ROW - INSERT (new.id=1)
AFTER EACH ROW - INSERT (new.id=1)
AFTER STATEMENT - INSERT 

1 row created.

SQL> INSERT INTO compound_trigger_test VALUES (2, 'TWO');
BEFORE STATEMENT - INSERT
BEFORE EACH ROW - INSERT (new.id=2)
AFTER EACH ROW - INSERT (new.id=2)
AFTER STATEMENT - INSERT 

1 row created.

SQL> UPDATE compound_trigger_test SET id = id;
BEFORE STATEMENT - UPDATE
BEFORE EACH ROW - UPDATE (new.id=2 old.id=2)
AFTER EACH ROW - UPDATE (new.id=2 old.id=2)
BEFORE EACH ROW - UPDATE (new.id=1 old.id=1)
AFTER EACH ROW - UPDATE (new.id=1 old.id=1)
AFTER STATEMENT - UPDATE

2 rows updated.

SQL> DELETE FROM compound_trigger_test;
BEFORE STATEMENT - DELETE
BEFORE EACH ROW - DELETE (old.id=2)
AFTER EACH ROW - DELETE (old.id=2)
BEFORE EACH ROW - DELETE (old.id=1)
AFTER EACH ROW - DELETE (old.id=1)
AFTER STATEMENT - DELETE

2 rows deleted. 

SQL>

One of the real advantages of a compound trigger is to use the FOR EACH ROW statement to capture changes and load them into a PL/SQL table, then use the AFTER STATEMENT statement to bulk load the changes into another table.  Using simple triggers, you would have to reference a package to load the data and then execute the bulk load.  With a compound trigger these operations are internal to the trigger.
-----------------------------------------------------------------------------------------------------------------------------------------------
NOCOPY Hint to Improve Performance of OUT and IN OUT Parameters in PL/SQL Code

Oracle has two methods of passing passing OUT and IN OUT parameters in PL/SQL code:

Pass By Value : The default action is to create a temporary buffer (formal parameter), copy the data from the parameter variable (actual parameter) to that buffer and work on the temporary buffer during the lifetime of the procedure. On successful completion of the procedure, the contents of the temporary buffer are copied back into the parameter variable. In the event of an exception occurring, the copy back operation does not happen.
Pass By Reference : Using the NOCOPY hint tells the compiler to use pass by reference, so no temporary buffer is needed and no copy forward and copy back operations happen. Instead, any modification to the parameter values are written directly to the parameter variable (actual parameter).
Under normal circumstances you probably wouldnt notice the difference between the two methods, but once you start to pass large or complex data types (LOBs, XMLTYPEs, collections etc.) the difference between the two methods can be come quite considerable. The presence of the temporary buffer means pass by value requires twice the memory for every OUT and IN OUT parameter, which can be a problem when using large parameters. In addition, the time it takes to copy the data to the temporary buffer and back to the parameter variable can be quite considerable.

PROCEDURE in_out_nocopy (p_tab  IN OUT NOCOPY  t_tab) IS
  l_count NUMBER;
BEGIN
  l_count := p_tab.count;
END in_out_nocopy;


Issues
There are a number of issues associated with using the NOCOPY hint that you should be aware of before adding it to all your OUT and IN OUT parameters.

NOCOPY is a hint. There are a number of circumstances where the compiler can ignore the hint, as described here.
If you are testing the contents of the parameter as a measure of successful completion of a procedure, adding NOCOPY may give unexpected results. For example, suppose I pass the value of NULL and assume if the parameter returns with a NOT NULL value the procedure has worked. This will work without NOCOPY, since the copy back operation will not happen in the event of an exception being raised. If I add NOCOPY, all changes are instantly written to the actual parameter, so exceptions will not prevent a NOT NULL value being returned. This may seem like a problem, but in my opinion if this affects you it is an indication of bad coding practice on your part. Failure should be indicated by raising an exception, or at worst using a status flag, rather than testing for values.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Table Functions and Pipelined Table Functions

Table Functions
Table functions are used to return PL/SQL collections that mimic tables. They can be queried like a regular table by using the TABLE operator in the FROM clause. Regular table functions require collections to be fully populated before they are returned. Since collections are held in memory, this can be a problem as large collections can waste a lot of memory and take a long time to return the first row. These potential bottlenecks make regular table functions unsuitable for large Extraction Transformation Load (ETL) operations. Regular table functions require named row and table types to be created as database objects.

Create the types to support the table function.
DROP TYPE t_tf_tab;
DROP TYPE t_tf_row;

CREATE TYPE t_tf_row AS OBJECT (
  id           NUMBER,
  description  VARCHAR2(50)
);
/

CREATE TYPE t_tf_tab IS TABLE OF t_tf_row;
/

-- Build the table function itself.
CREATE OR REPLACE FUNCTION get_tab_tf (p_rows IN NUMBER) RETURN t_tf_tab AS
  l_tab  t_tf_tab := t_tf_tab();
BEGIN
  FOR i IN 1 .. p_rows LOOP
    l_tab.extend;
    l_tab(l_tab.last) := t_tf_row(i, 'Description for ' || i);
  END LOOP;

  RETURN l_tab;
END;
/

-- Test it.
SELECT *
FROM   TABLE(get_tab_tf(10))
ORDER BY id DESC;

        ID DESCRIPTION
---------- --------------------------------------------------
        10 Description for 10
         9 Description for 9
         8 Description for 8
         7 Description for 7
         6 Description for 6
         5 Description for 5
         4 Description for 4
         3 Description for 3
         2 Description for 2
         1 Description for 1

10 rows selected.

PIPELINED TABLE FUNCTIONS  --advantage memory usage over regular tables
Pipelining negates the need to build huge collections by piping rows out of the function as they are created, saving memory and allowing subsequent processing to start before all the rows are generated.

Pipelined table functions include the PIPELINED clause and use the PIPE ROW call to push rows out of the function as soon as they are created, rather than building up a table collection. Notice the empty RETURN call, since there is no collection to return from the function.

-- Build a pipelined table function.
CREATE OR REPLACE FUNCTION get_tab_ptf (p_rows IN NUMBER) RETURN t_tf_tab PIPELINED AS
BEGIN
  FOR i IN 1 .. p_rows LOOP
    PIPE ROW(t_tf_row(i, 'Description for ' || i));   
  END LOOP;

  RETURN;
END;
/

-- Test it.
SELECT *
FROM   TABLE(get_tab_ptf(10))
ORDER BY id DESC;

        ID DESCRIPTION
---------- --------------------------------------------------
        10 Description for 10
         9 Description for 9
         8 Description for 8
         7 Description for 7
         6 Description for 6
         5 Description for 5
         4 Description for 4
         3 Description for 3
         2 Description for 2
         1 Description for 1

10 rows selected.

Once you start working with large warehousing ETL operations the performance improvements can be massive, allowing data loads from external tables via table functions directly into the warehouse tables, rather than loading via a staging area.

NO_DATA_NEEDED Exception
A pipelined table function may create more data than is needed by the process querying it. When this happens, the pipelined table function execution stops, raising the NO_DATA_NEEDED exception. This doesn't need to be explicitly handled provided you do not include an OTHERS exception handler.

The function below returns 10 rows, but the query against it only ask for the first 5 rows, so the function stops processing by raising the NO_DATA_NEEDED exception.

-- Build a pipelined table function.
CREATE OR REPLACE FUNCTION get_tab_ptf (p_rows IN NUMBER) RETURN t_tf_tab PIPELINED AS
BEGIN
  FOR i IN 1 .. p_rows LOOP
    DBMS_OUTPUT.put_line('Row: ' || i);
    PIPE ROW(t_tf_row(i, 'Description for ' || i));
  END LOOP;

  RETURN;
END;
/

Parallel Enabled Pipelined Table Functions
To parallel enable a pipelined table function the following conditions must be met.

The PARALLEL_ENABLE clause must be included.
It must have one or more REF CURSOR input parameters.
It must have a PARTITION BY clause to define a partitioning method for the workload. Weakly typed ref cursors can only use the PARTITION BY ANY clause, which randomly partitions the workload.
The basic syntax is shown below.

CREATE FUNCTION function-name(parameter-name ref-cursor-type)
  RETURN rec_tab_type PIPELINED
  PARALLEL_ENABLE(PARTITION parameter-name BY [{HASH | RANGE} (column-list) | ANY ]) IS
BEGIN
  ...
END;

note: Pipelined functions are useful if there is a need for a data source other than a table in a select statement
A function is considered deterministic if it always returns the same result for a specific input value.  The Oracle documentation claims that defining pipelined table functions as deterministic by using the DETERMINISTIC clause allows Oracle to buffer their rows, thereby preventing multiple executions. But I can find no evidence to support this claim. 

The test_deterministic.sql script defines a package containing two pipelined table functions, one of which is defined as deterministic.  It then executes the functions multiple times using two different methods to compare their performance.
---------------------------------------------------------------------
What does the dbms_parallel_execute DBMS package do?  When would I use dbms_parallel_execute instead of parallel DML?  Is it superior to parallel DML?

 No, the dbms_parallel_execute package (new in 11g r2) is not as easy to use as vanilla parallel DML, IMHO, because Oracle parallel query (and DML) reliably divides a table into equal-sized chunks automatically.  The dbms_parallel_execute package is for special cases of incremental bulk updates in PL/SQL.

you should use this package only where straight parallel DML is not appropriate, such as when you are using parallel DML inside PL/SQL loops (such as incrementally bulk updating data). This do-it-yourself parallelism in dbms_parallel_execute can be used to manually specify the degree of parallelism to fully saturate a dedicated server and he notes the following steps for using the dbms_parallel_execute package:
 
1:  Create a task

2:  Split the workload into chunks

   exec dbms_parallel_execute.create_chunks_by_rowid
   exec dbms_parallel_execute.create_chunks_by_number_col
   exec dbms_parallel_execute.create_chunks_by_sql
 
3:  Run the task

   exec dbms_parallel_execute.run_task
   User-defined framework
   Task control
 
4:  Check the task status

5:  Drop the task
 
You can view the status of dbms_parallel_execute with this dictionary SQL against the dba_parallel_execute_chunks view:
 
select 
   chunk_id, 
   status, 
   start_id, 
   end_id
from   
   dba_parallel_execute_chunks
where  
   task_name = 'test_task'
order by 
   chunk_id;
   
 used to Updating large tables in parallel
-------------------------------------------------------------------------------------------------------------------------------

---oracle logging and no logging

The logging_clause lets you specify whether creation of a database object will be logged in the redo log file (LOGGING) or not (NOLOGGING).

The NOLOGGING attribute tells the Oracle that the operation being performed does not need to be recoverable in the event of a failure. In this case Oracle will generate a minimal number of redo log entries in order to protect the data dictionary, and the operation will probably run faster. Oracle is relying on the user to recover the data manually in the event of a media failure













